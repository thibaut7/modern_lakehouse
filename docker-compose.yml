version: '3.8'

services:
  # Object Storage (S3 Compatible)
  minio:
    image: minio/minio:latest
    container_name: lakehouse-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - lakehouse-net

  # Create buckets automatically
  minio-create-buckets:
    image: minio/mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}); do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc mb --ignore-existing myminio/raw-data;
      /usr/bin/mc mb --ignore-existing myminio/warehouse;
      echo 'Buckets created';
      "
    networks:
      - lakehouse-net

  # Database (Datamart + Catalog)
  postgres:
    image: postgres:15
    container_name: lakehouse-postgres
    ports:
      - "5435:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./postgres/init-scripts:/docker-entrypoint-initdb.d
      - postgres_data:/var/lib/postgresql/data
    networks:
      - lakehouse-net

  # Ingestion Tool
  nifi:
    image: apache/nifi:latest
    container_name: lakehouse-nifi
    ports:
      - "8443:8443" # Secure UI
    environment:
      - NIFI_WEB_HTTP_PORT=
      - NIFI_WEB_HTTPS_PORT=8443
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=password12345678  # Needs to be >= 12 chars
    volumes:
      - nifi_data:/opt/nifi/nifi-current/data
      - nifi_conf:/opt/nifi/nifi-current/conf
    networks:
      - lakehouse-net

  # Spark Master
  spark-master:
    build: 
      context: ./spark
      dockerfile: Dockerfile
    container_name: lakehouse-spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8085:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # AWS/S3 Config
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_REGION=us-east-1
      # Connectors are built-in via Dockerfile, but can be added here as packages too.
      # Ideally we use spark-defaults.conf or pass packages in submit command.
    volumes:
      - ./spark:/opt/bitnami/spark/jobs
    networks:
      - lakehouse-net

  # Spark Worker
  spark-worker:
    build: 
      context: ./spark
      dockerfile: Dockerfile
    container_name: lakehouse-spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_MASTER_URL=spark://spark-master:7077
      # AWS/S3 Config
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_REGION=us-east-1
    volumes:
      - ./spark:/opt/bitnami/spark/jobs
    networks:
      - lakehouse-net

networks:
  lakehouse-net:
    driver: bridge

volumes:
  minio_data:
  postgres_data:
  nifi_data:
  nifi_conf:
